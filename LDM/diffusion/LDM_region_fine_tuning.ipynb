{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, Dataset\n",
    "import os, random, sys, yaml\n",
    "import torch, torchvision, netCDF4\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "sys.path.append(r'/root/lwd/SDM_program')\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "def read_yaml(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        data = yaml.safe_load(file)\n",
    "    return data\n",
    "opt = read_yaml('/root/lwd/SDM_program/opt.yaml')\n",
    "\n",
    "sys.path.append(opt['path']['model_save_path'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2016\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1/1096 [00:00<02:00,  9.10it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1096/1096 [01:55<00:00,  9.48it/s]\n"
     ]
    }
   ],
   "source": [
    "# dataset \n",
    "from datetime import datetime, timedelta\n",
    "from dataset import random_sample_fixed\n",
    "print(opt['dataset']['sart_date'])\n",
    "start_date = datetime(opt['dataset']['sart_date'], 1, 1)\n",
    "end_date = datetime(opt['dataset']['end_date'], 12, 31)\n",
    "current_date = start_date\n",
    "date_list = []\n",
    "while current_date <= end_date:\n",
    "    date_list.append(current_date.strftime(\"%Y%m%d\"))\n",
    "    current_date += timedelta(days=1)\n",
    "\n",
    "lats_lr, lons_lr = np.arange(50, 24.5, -0.25), np.arange(235, 293.5, 0.25)\n",
    "lats_sr, lons_sr = np.arange(50, 24.5, -1/24), np.arange(235, 293.5, 1/24)\n",
    "region = 'A'\n",
    "if region == 'A':\n",
    "    random_lon = 241\n",
    "    random_lat = 38\n",
    "elif region == 'B':\n",
    "    random_lon = 269\n",
    "    random_lat = 32\n",
    "elif region == 'C':\n",
    "    random_lon = 269\n",
    "    random_lat = 43\n",
    "lon_lr_idx = np.where(lons_lr == random_lon)[0][0]\n",
    "lat_lr_idx = np.where(lats_lr == random_lat)[0][0]\n",
    "'''\n",
    "I stored the region data separately as nc file. \n",
    "If not, the random_sample_fixed function was used to cut out the regional patch from CONUS\n",
    "'''\n",
    "condition = {}\n",
    "prism = {}\n",
    "for current_date in tqdm(date_list):\n",
    "    # condition: 'z', 'q', 't', 'u', 'v'\n",
    "    path_meteo = r'/dynamical_input/meteo_%s.nc' % current_date\n",
    "    with netCDF4.Dataset(opt['path']['path_data_root'] + path_meteo) as ds:\n",
    "        v_meteo = ds.variables['data'][:].data\n",
    "    path_t2m = r'/T2M/t2m_%s.nc' % current_date\n",
    "    with netCDF4.Dataset(opt['path']['path_data_root'] + path_t2m) as ds:\n",
    "        t2m_1d = ds.variables['data'][:].data\n",
    "\n",
    "    # dynamical_input = random_sample_fixed(np.concatenate((v_meteo[:-24*3], t2m_1d), axis=0), \n",
    "    #                                       lon_lr_idx, lat_lr_idx, r=24)\n",
    "    dynamical_input = np.concatenate((v_meteo, t2m_1d), axis=0)\n",
    "    condition[current_date] = dynamical_input\n",
    "    del dynamical_input, v_meteo, t2m_1d\n",
    "    # condition with meteo without t2m\n",
    "    # path_meteo = r'/dynamical_input/meteo_%s.nc' % current_date\n",
    "    # v_meteo = netCDF4.Dataset(opt['path']['path_data_root'] + path_meteo).variables['data'][:].data\n",
    "    # condition[current_date] = v_meteo\n",
    "    # precip\n",
    "    path_pr_sr = r'/pr_sr_nc/pr_sr_%s.nc' % current_date\n",
    "    pr_sr = netCDF4.Dataset(opt['path']['path_data_root'] + path_pr_sr).variables['data'][:].data\n",
    "    # pr_sr = random_sample_fixed(pr_sr, lon_lr_idx*6, lat_lr_idx*6, r=96)\n",
    "    pr_sr[pr_sr==-9999] = np.nan\n",
    "    pr_sr[np.isnan(pr_sr)] = 0\n",
    "    prism[current_date] = pr_sr\n",
    "    del pr_sr\n",
    "\n",
    "# condition['h'] = netCDF4.Dataset(opt['path']['path_data_root'] + r'/usa_dem.nc').variables['data'][:].data\n",
    "condition['h'] = random_sample_fixed(np.load(opt['path']['path_data_root'] + r'/dem_us_lr.npy'), \n",
    "                                          lon_lr_idx, lat_lr_idx, r=24)\n",
    "# sample 8°*8° patch 10 times on one day from the CONUS\n",
    "data_list = [date for date in date_list for _ in range(10)]\n",
    "data_len = len(data_list)\n",
    "train_data_list = data_list[:int(data_len*0.9)]\n",
    "val_data_list = data_list[int(data_len*0.9):int(data_len)]\n",
    "\n",
    "class dicDataset(Dataset):\n",
    "    def __init__(self, data_list, batch_size=64):\n",
    "        self.data_list = data_list\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data_list)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "\n",
    "        pr_transformed = np.log(prism[self.data_list[index]] + 1)[np.newaxis, :, :]\n",
    "        \n",
    "        condition_day_sampled = condition[self.data_list[index]]\n",
    "        condition_dem_sampled = condition['h'][np.newaxis, :, :]\n",
    "        condition_sampled = np.concatenate((condition_day_sampled, condition_dem_sampled), axis=0)\n",
    "\n",
    "        return {'pr':torch.from_numpy(pr_transformed), \n",
    "                'condition':torch.from_numpy(condition_sampled)}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset =dicDataset(train_data_list)\n",
    "val_dataset = dicDataset(val_data_list)\n",
    "batch_size = opt['train']['batch_size']\n",
    "num_workers=8\n",
    "train_dataloader = DataLoader(dataset=train_dataset, batch_size=batch_size,\n",
    "                    shuffle=True, pin_memory=True, num_workers=num_workers) \n",
    "val_dataloader = DataLoader(dataset=val_dataset, batch_size=batch_size,\n",
    "                    shuffle=True, pin_memory=True, num_workers=num_workers)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load VAE and cUNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResUnet(\n",
       "  (init_conv): ResidualConvBlock(\n",
       "    (conv1): Sequential(\n",
       "      (0): Conv2d(385, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): GELU(approximate='none')\n",
       "      (3): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (conv2): Sequential(\n",
       "      (0): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): GELU(approximate='none')\n",
       "    )\n",
       "  )\n",
       "  (down1): UnetDown(\n",
       "    (model): Sequential(\n",
       "      (0): ResidualConvBlock(\n",
       "        (conv1): Sequential(\n",
       "          (0): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): GELU(approximate='none')\n",
       "          (3): Identity()\n",
       "        )\n",
       "        (conv2): Sequential(\n",
       "          (0): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): GELU(approximate='none')\n",
       "        )\n",
       "      )\n",
       "      (1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    )\n",
       "  )\n",
       "  (down2): UnetDown(\n",
       "    (model): Sequential(\n",
       "      (0): ResidualConvBlock(\n",
       "        (conv1): Sequential(\n",
       "          (0): Conv2d(1024, 2048, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): GELU(approximate='none')\n",
       "          (3): Identity()\n",
       "        )\n",
       "        (conv2): Sequential(\n",
       "          (0): Conv2d(2048, 2048, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): GELU(approximate='none')\n",
       "        )\n",
       "      )\n",
       "      (1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    )\n",
       "  )\n",
       "  (to_vec): Sequential(\n",
       "    (0): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
       "    (1): GELU(approximate='none')\n",
       "  )\n",
       "  (timeembed1): EmbedFC(\n",
       "    (model): Sequential(\n",
       "      (0): Linear(in_features=1, out_features=2048, bias=True)\n",
       "      (1): GELU(approximate='none')\n",
       "      (2): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (timeembed2): EmbedFC(\n",
       "    (model): Sequential(\n",
       "      (0): Linear(in_features=1, out_features=1024, bias=True)\n",
       "      (1): GELU(approximate='none')\n",
       "      (2): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (up0): Sequential(\n",
       "    (0): ConvTranspose2d(2048, 2048, kernel_size=(2, 2), stride=(2, 2))\n",
       "    (1): GroupNorm(8, 2048, eps=1e-05, affine=True)\n",
       "    (2): ReLU()\n",
       "  )\n",
       "  (up1): UnetUp(\n",
       "    (model): Sequential(\n",
       "      (0): ConvTranspose2d(4096, 1024, kernel_size=(2, 2), stride=(2, 2))\n",
       "      (1): ResidualConvBlock(\n",
       "        (conv1): Sequential(\n",
       "          (0): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): GELU(approximate='none')\n",
       "          (3): Identity()\n",
       "        )\n",
       "        (conv2): Sequential(\n",
       "          (0): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): GELU(approximate='none')\n",
       "        )\n",
       "      )\n",
       "      (2): ResidualConvBlock(\n",
       "        (conv1): Sequential(\n",
       "          (0): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): GELU(approximate='none')\n",
       "          (3): Identity()\n",
       "        )\n",
       "        (conv2): Sequential(\n",
       "          (0): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): GELU(approximate='none')\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (up2): UnetUp(\n",
       "    (model): Sequential(\n",
       "      (0): ConvTranspose2d(2048, 1024, kernel_size=(2, 2), stride=(2, 2))\n",
       "      (1): ResidualConvBlock(\n",
       "        (conv1): Sequential(\n",
       "          (0): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): GELU(approximate='none')\n",
       "          (3): Identity()\n",
       "        )\n",
       "        (conv2): Sequential(\n",
       "          (0): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): GELU(approximate='none')\n",
       "        )\n",
       "      )\n",
       "      (2): ResidualConvBlock(\n",
       "        (conv1): Sequential(\n",
       "          (0): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): GELU(approximate='none')\n",
       "          (3): Identity()\n",
       "        )\n",
       "        (conv2): Sequential(\n",
       "          (0): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): GELU(approximate='none')\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (out): Sequential(\n",
       "    (0): Conv2d(2048, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): GroupNorm(8, 1024, eps=1e-05, affine=True)\n",
       "    (2): ReLU()\n",
       "    (3): Conv2d(1024, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# VAE\n",
    "from vae_precip import VAE\n",
    "num_variables = 1\n",
    "num_hiddens = 256 \n",
    "num_residual_layers = 2 \n",
    "num_residual_hiddens = 48  \n",
    "embedding_dim = 4 \n",
    "vae = VAE(num_variables, num_hiddens, num_residual_layers, num_residual_hiddens, \n",
    "                 embedding_dim).to(device)\n",
    "vae.load_state_dict(torch.load(opt['path']['model_save_path'] + 'vae_precip.pt')['model_state_dict'])\n",
    "vae.eval()\n",
    "\n",
    "# cUNet\n",
    "from cUNet import ResUnet\n",
    "cUNet = ResUnet(in_channels=opt['cUNet']['in_channels'], out_channels=opt['cUNet']['out_channels'], \n",
    "                n_feat=opt['cUNet']['n_feat'])\n",
    "cUNet.to(device)\n",
    "cUNet.load_state_dict(torch.load(opt['path']['model_save_path'] + 'UNet_con.pt')['model_state_dict'])\n",
    "cUNet.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load DDPM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from /root/autodl-tmp/model_save/SDM/SDM_best_train.pt\n"
     ]
    }
   ],
   "source": [
    "# ddpm\n",
    "from diffusion.ddpm import DDPM_cfg\n",
    "ddpm = DDPM_cfg(opt)\n",
    "optG = torch.optim.Adam(list(ddpm.parameters()), lr=opt['train'][\"optimizer\"][\"lr\"])\n",
    "\n",
    "train_loss_list, val_loss_list = [], []\n",
    "best_val_loss = float('inf') \n",
    "patience = 200  \n",
    "counter = 0 \n",
    "best_train_path = opt['path']['model_save_path'] + 'SDM_best_train.pt'\n",
    "\n",
    "# load ddpm\n",
    "load_path = opt['path']['model_save_path'] + 'SDM_gen.pt'\n",
    "load_path_train = opt['path']['model_save_path'] + 'SDM_best_train.pt'\n",
    "if os.path.exists(load_path):\n",
    "    print('Loading model from {}'.format(load_path))\n",
    "    checkpoint = torch.load(load_path)\n",
    "    ddpm.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optG.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "elif os.path.exists(load_path_train):\n",
    "    print('Loading model from {}'.format(load_path_train))\n",
    "    checkpoint = torch.load(load_path_train)\n",
    "    ddpm.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optG.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "else:\n",
    "    print('the first time to train')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1200 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train loss: 0.0158; val loss: 0.0072:   0%|          | 0/1200 [05:38<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best_val_loss: 0.0071658453363592604\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train loss: 0.0218; val loss: 0.0102:   3%|▎         | 36/1200 [3:24:52<110:24:24, 341.46s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 27\u001b[0m\n\u001b[1;32m     24\u001b[0m     optG\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;66;03m# pbar.set_description(f\"train loss: {l_pix.item():.4f}\")\u001b[39;00m\n\u001b[0;32m---> 27\u001b[0m     epoch_loss_list\u001b[38;5;241m.\u001b[39mappend(\u001b[43ml_pix\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     28\u001b[0m train_mean_loss \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmean(epoch_loss_list)\n\u001b[1;32m     29\u001b[0m train_loss_list\u001b[38;5;241m.\u001b[39mappend(train_mean_loss)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# train\n",
    "lrate = opt['train']['optimizer']['lr']\n",
    "n_epoch = opt['train']['n_epoch']\n",
    "x_p_c = {}\n",
    "pbar = tqdm(range(n_epoch))\n",
    "for epoch in pbar:\n",
    "    ddpm.train()\n",
    "    epoch_loss_list = []\n",
    "    # optG.param_groups[0]['lr'] = lrate*(1-epoch/(n_epoch+1))/2\n",
    "    # pbar = tqdm(train_dataloader)\n",
    "    for dictdata in train_dataloader:\n",
    "        optG.zero_grad()\n",
    "        dictdata['pr'] = torch.as_tensor(dictdata['pr'])\n",
    "        dictdata['pr'] = dictdata['pr'].to(device)\n",
    "        z = vae.encoder(dictdata['pr'])\n",
    "        z = vae._pre_vq_conv(z)\n",
    "\n",
    "        x_p_c['pr'] = z\n",
    "        dictdata['condition'] = torch.as_tensor(dictdata['condition'])\n",
    "        x_p_c['condition'] = cUNet(dictdata['condition'].to(device))\n",
    "        l_pix = ddpm(x_p_c) \n",
    "        l_pix.backward()\n",
    "        optG.step()\n",
    "\n",
    "        epoch_loss_list.append(l_pix.item())\n",
    "    train_mean_loss = np.mean(epoch_loss_list)\n",
    "    train_loss_list.append(train_mean_loss)\n",
    "    \n",
    "    # val\n",
    "    epoch_loss_list = []\n",
    "    ddpm.eval()\n",
    "    with torch.no_grad():\n",
    "        # pbar1 = tqdm(val_dataloader)\n",
    "        for dictdata in val_dataloader:\n",
    "            dictdata['pr'] = torch.as_tensor(dictdata['pr'])\n",
    "            dictdata['pr'] = dictdata['pr'].to(device)\n",
    "            z = vae.encoder(dictdata['pr'])\n",
    "            z = vae._pre_vq_conv(z)\n",
    "\n",
    "            x_p_c['pr'] = z\n",
    "            dictdata['dictdata'] = torch.as_tensor(dictdata['condition'])\n",
    "            x_p_c['condition'] = cUNet(dictdata['condition'].to(device))\n",
    "            l_pix = ddpm(x_p_c) # self.netG(self.data) # loss of pixel\n",
    "            \n",
    "            # pbar1.set_description(f\"val loss: {l_pix.item():.4f}\")\n",
    "            epoch_loss_list.append(l_pix.item())\n",
    "    val_mean_loss = np.mean(epoch_loss_list)\n",
    "    val_loss_list.append(val_mean_loss)\n",
    "    pbar.set_description(f\"train loss: {train_mean_loss:.4f}; val loss: {val_mean_loss:.4f}\")\n",
    "\n",
    "    if val_mean_loss < best_val_loss:\n",
    "        best_val_loss = val_mean_loss\n",
    "        counter = 0\n",
    "        state_dict = ddpm.state_dict()\n",
    "        optimizer_state_dict = optG.state_dict()\n",
    "        for key, param in state_dict.items():\n",
    "            state_dict[key] = param.cpu()\n",
    "        torch.save({\n",
    "            'model_state_dict': state_dict,\n",
    "            'optimizer_state_dict': optimizer_state_dict,}, best_train_path)\n",
    "        print('best_val_loss:', best_val_loss)\n",
    "    else:\n",
    "        counter += 1\n",
    "\n",
    "    if counter >= patience:\n",
    "        print(\"Best val loss is {}, No improvement in val loss for {} epochs.\".format(best_val_loss, patience))\n",
    "    if (epoch+1) % 10:\n",
    "        state_dict = ddpm.state_dict()\n",
    "        optimizer_state_dict = optG.state_dict()\n",
    "        for key, param in state_dict.items():\n",
    "            state_dict[key] = param.cpu()\n",
    "        torch.save({\n",
    "            'model_state_dict': state_dict,\n",
    "            'optimizer_state_dict': optimizer_state_dict,}, best_train_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_path = opt['path']['model_save_path'] + 'SDM_gen.pt'\n",
    "# gen\n",
    "state_dict = ddpm.state_dict()\n",
    "optimizer_state_dict = optG.state_dict()\n",
    "for key, param in state_dict.items():\n",
    "    state_dict[key] = param.cpu()\n",
    "torch.save({\n",
    "    'model_state_dict': state_dict, \n",
    "    'optimizer_state_dict': optimizer_state_dict,}, gen_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
